From 9cdcd3f12fc513d39132d896a76735532e7eee5a Mon Sep 17 00:00:00 2001
From: liuyanwei <liuyanwei6@huawei.com>
Date: Tue, 18 Mar 2025 14:44:30 +0800
Subject: [PATCH] modified mindformers

---
 mindformers/core/callback/callback.py    |  4 +-
 mindformers/modules/transformer/moev3.py | 60 +++++++++++++++---------
 2 files changed, 40 insertions(+), 24 deletions(-)

diff --git a/mindformers/core/callback/callback.py b/mindformers/core/callback/callback.py
index 75d56991..bdd9dcd1 100644
--- a/mindformers/core/callback/callback.py
+++ b/mindformers/core/callback/callback.py
@@ -372,10 +372,10 @@ class MFLossMonitor(Callback):
                                    overflow, scaling_sens, time_remain, percent, global_norm)
 
         if check_in_modelarts() and get_real_rank() == get_real_group_size() - 1:
-            self.dump_info_to_modelarts(ma_step_num=cur_step_num, ma_loss=loss)
-
+            self.dump_info_to_modelarts(ma_step_num=cur_step_num, ma_loss=loss)
         if auto_parallel:
             set_auto_parallel_context(parallel_mode=parallel_mode, full_batch=full_batch)
+        run_context.request_stop()
 
     def _fix_loss_for_parallel(self, loss):
         """Fix loss value in pipeline or double parallel mode."""
diff --git a/mindformers/modules/transformer/moev3.py b/mindformers/modules/transformer/moev3.py
index 210db93d..b40a52db 100644
--- a/mindformers/modules/transformer/moev3.py
+++ b/mindformers/modules/transformer/moev3.py
@@ -400,6 +400,13 @@ def nddispatch(tokens, expert_ids, router_coeff, ep, expert_num):
     router_coeff = ops.gather(router_coeff, dispatch_idx, axis=0, batch_dims=0)
     return tokens, expert_ids, router_coeff, dispatch_idx, counter
 
+E = 16
+topk = 8
+mp = 2
+seqlen = 4096
+oep = 2
+inner_seq = seqlen * topk // mp
+IEP = 2
 
 def get_exdispatch_idx(x, expert_ids, router_coeff, a, b, oep_group):
     """
@@ -419,7 +426,8 @@ def get_exdispatch_idx(x, expert_ids, router_coeff, a, b, oep_group):
     mask = ops.logical_and(sorted_expert_ids >= a, sorted_expert_ids < b)
     x = ops.Depend()(x, mask)
     x = ops.AllGather(group=oep_group)(x)
-    idx = mask.reshape(-1).nonzero()
+    # idx = mask.reshape(-1).nonzero()
+    idx = ops.range(0, inner_seq * oep, 1)
     idx = idx.reshape(-1)
     dispatch_idx = ops.gather(dispatch_idx_floordiv_k,
                               idx, axis=0, batch_dims=0)
@@ -465,7 +473,10 @@ def ffn_forward_func(x, expert_id, counter, w1, w2, ep_group, hidden_size, ep, u
     # prepare sl, rl, gl.
     # they should be calculated dynamically from the real expert_id and counters
     x_shape_origin = x.shape
-    local_counter = ops.AlltoAll(split_count=ep, split_dim=-1, concat_dim=-2, group=ep_group)(counter)
+    E, topk, mp, seqlen = 16, 8, 2, 4096
+    tokens = seqlen // mp * topk // E
+    counter = Tensor([tokens] * E, ms.float32).reshape(ep, -1)
+    local_counter = counter
     # [ep, E/ep] -->  [ep]
     send_list = ops.cast(counter.reshape(ep, -1).sum(dim=-1, keepdim=False), ms.int64)
     # [ep, E/ep]  --> [ep]
@@ -473,10 +484,10 @@ def ffn_forward_func(x, expert_id, counter, w1, w2, ep_group, hidden_size, ep, u
     # [ep, E/ep]  --> (E/ep) int64
     group_list = ops.cast(ops.cumsum(local_counter.reshape(ep, -1).sum(dim=-2, keepdim=False), 0), ms.int64)
 
-    send_list = ops.Depend()(send_list, receive_list)
-    send_list = ops.Depend()(send_list, group_list)
-    send_list = D2H(send_list, "CPU", True)
-    receive_list = D2H(receive_list, "CPU", True)
+    # send_list = ops.Depend()(send_list, receive_list)
+    # send_list = ops.Depend()(send_list, group_list)
+    # send_list = D2H(send_list, "CPU", True)
+    # receive_list = D2H(receive_list, "CPU", True)
 
     # 1.AllToAllv
     # x [B, S, h]
@@ -490,7 +501,7 @@ def ffn_forward_func(x, expert_id, counter, w1, w2, ep_group, hidden_size, ep, u
     x, unresort_map = _ffn_resort(x, expert_id, use_fused_ops_permute)
 
     # 3.GroupedMM
-    # squeeze x [B, S, h] -- > [B*S, h] where B=1
+    # squeeze x [B, S, h] -- > [B*S, h] where B=1  [e, h, H]
     x = x.reshape((-1, hidden_size))
     gate = GroupedMatmul(split_item=3, group_type=0)([x], [w1], None, None, None, None, None, group_list)[0]
     # pylint: disable=W0212
@@ -658,17 +669,20 @@ def ffn_forward_deredundency_func(x, expert_id, router_coeff, w1, w2, iep, exper
     excounter = excounter.sum(axis=0)[a:b]
     local_excounter = ops.AlltoAllV(group=iep_group, block_size=1)(
         excounter.reshape(-1), iepones, iepones)
-    exrl = ops.cast(local_excounter.reshape(
-        iep, -1).sum(axis=1), ms.int64)  # [outer_ep]
-    exgl = ops.cast(ops.cumsum(local_excounter.reshape(
-        iep, -1).sum(dim=-2, keepdim=False), 0, ms.int32), ms.int64)
-    exsl = ops.cast(excounter.reshape(
-        iep, -1).sum(axis=1), ms.int64)  # [outer_ep]
-    exgl = ops.Depend()(exgl, exrl)
-    exsl = ops.Depend()(exsl, exgl)
-
-    router_coeff = ops.Depend()(router_coeff, exgl)
-    expert_id = ops.Depend()(expert_id, exsl)
+    # exrl = ops.cast(local_excounter.reshape(
+    #     iep, -1).sum(axis=1), ms.int64)  # [outer_ep]
+    # exgl = ops.cast(ops.cumsum(local_excounter.reshape(
+    #     iep, -1).sum(dim=-2, keepdim=False), 0, ms.int32), ms.int64)
+    # exsl = ops.cast(excounter.reshape(
+    #     iep, -1).sum(axis=1), ms.int64)  # [outer_ep]
+    exrl = [inner_seq] * IEP
+    exgl = Tensor([seqlen] * (E // IEP // oep), dtype=ms.int64)
+    exsl = [inner_seq] * IEP
+    # exgl = ops.Depend()(exgl, exrl)
+    # exsl = ops.Depend()(exsl, exgl)
+
+    # router_coeff = ops.Depend()(router_coeff, exgl)
+    # expert_id = ops.Depend()(expert_id, exsl)
 
     # 1. allgather
     router_coeff = ops.AllGather(group=oep_group)(router_coeff).reshape(-1, chosen_expert_nums)
@@ -676,10 +690,10 @@ def ffn_forward_deredundency_func(x, expert_id, router_coeff, w1, w2, iep, exper
     # 2. exdispatch
     x, exdispatch_idx, expert_id, router_coeff = get_exdispatch_idx(
         x, expert_id, router_coeff, a, b, oep_group)
-    exgl = ops.Depend()(exgl, x)
-    exsl = ops.Depend()(exsl, exdispatch_idx)
-    exsl = D2H(exsl, "CPU", True)
-    exrl = D2H(exrl, "CPU", True)
+    # exgl = ops.Depend()(exgl, x)
+    # exsl = ops.Depend()(exsl, exdispatch_idx)
+    # exsl = D2H(exsl, "CPU", True)
+    # exrl = D2H(exrl, "CPU", True)
     excombine_whiteboard = x * Tensor(0.0, dtype=ms.bfloat16)
     x = ops.gather(x, exdispatch_idx, axis=0, batch_dims=0)
 
@@ -707,6 +721,8 @@ def ffn_forward_deredundency_func(x, expert_id, router_coeff, w1, w2, iep, exper
     # -4. unresort
     x = ops.gather(x, unsort_map, axis=0, batch_dims=0)
 
+    exrl = [inner_seq] * IEP
+    exsl = [inner_seq] * IEP
     # -3. allToAllv
     x = ops.AlltoAllV(group=iep_group, block_size=hidden_size)(
         x.reshape(-1), exrl, exsl).reshape(-1, hidden_size)
-- 
2.27.0
