all tp gourps [[0, 1], [2, 3], [4, 5], [6, 7]]
all ep groups [[0], [1], [2], [3], [4], [5], [6], [7]]
all dp groups [[0], [1], [2], [3], [4], [5], [6], [7]]
all_dp_modulo_exp_group_ranks [[0], [1], [2], [3], [4], [5], [6], [7]]
all_tensor_and_expert_group_ranks [[0, 1], [2, 3], [4, 5], [6, 7]]
all_data_parallel_group_ranks_with_cp [[0], [1], [2], [3], [4], [5], [6], [7]]
> initialized tensor model parallel with size 2
> initialized pipeline model parallel with size 4
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/home/z00617129/code/test_case/pta/Megatron-LM/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/z00617129/code/test_case/pta/Megatron-LM/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.125 seconds
time to initialize megatron (seconds): 76.366
building GPT model ...
> learning rate decay style: cosine
 loading checkpoint from /home/workspace/mindspore_dataset/msadapter/test_input/net/test_qwen_sft/ckpt at iteration 10
could not find arguments in the checkpoint ...
 checkpoint version 3.0
  successfully loaded checkpoint from /home/workspace/mindspore_dataset/msadapter/test_input/net/test_qwen_sft/ckpt [ t 0, p 0 ] at iteration 0
(min, max) time across ranks (ms):
    load-checkpoint ................................: (4470.81, 4473.36)
[after model, optimizer, and learning rate scheduler are built] datetime: 2025-03-20 14:27:30 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1280
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 1.0), None, None]
 > loading shuffle-idx mapping from /home/workspace/mindspore_dataset/msadapter/test_input/net/test_qwen_sft/dataset/finetune_dataset/alpaca_train_indexmap_1280ns_1234s_decoder_packed_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2025-03-20 14:27:31 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (6383.55, 6456.24)
    train/valid/test-data-iterators-setup ..........: (874.57, 897.81)
training ...
[before the start of training step] datetime: 2025-03-20 14:27:32 
 [2025-03-20 14:27:50] iteration        1/      10 | consumed samples:          128 | elapsed time per iteration (ms): 18062.4 | throughput per GPU (TFLOP/s/GPU): 379.4 | learning rate: 1.235606E-06 | global batch size:   128 | lm loss: 9.7951211929321289 | loss scale: 1.0 | grad norm: 20.1497642787274636 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-20 14:27:54] iteration        2/      10 | consumed samples:          256 | elapsed time per iteration (ms): 4235.8 | throughput per GPU (TFLOP/s/GPU): 1617.9 | learning rate: 1.166872E-06 | global batch size:   128 | lm loss: 9.7579383850097656 | loss scale: 1.0 | grad norm: 23.7711511490950933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-20 14:27:50] iteration        3/      10 | consumed samples:          128 | elapsed time per iteration (ms): 18062.4 | throughput per GPU (TFLOP/s/GPU): 379.4 | learning rate: 1.235606E-06 | global batch size:   128 | lm loss: 9.7951211929321289 | loss scale: 1.0 | grad norm: 20.1497642787274636 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-20 14:27:54] iteration        4/      10 | consumed samples:          256 | elapsed time per iteration (ms): 4235.8 | throughput per GPU (TFLOP/s/GPU): 1617.9 | learning rate: 1.166872E-06 | global batch size:   128 | lm loss: 9.7579383850097656 | loss scale: 1.0 | grad norm: 23.7711511490950933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-20 14:27:50] iteration        5/      10 | consumed samples:          128 | elapsed time per iteration (ms): 18062.4 | throughput per GPU (TFLOP/s/GPU): 379.4 | learning rate: 1.235606E-06 | global batch size:   128 | lm loss: 9.7951211929321289 | loss scale: 1.0 | grad norm: 20.1497642787274636 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2025-03-20 14:28:26