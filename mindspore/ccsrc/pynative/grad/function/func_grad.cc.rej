diff a/mindspore/ccsrc/pynative/grad/function/func_grad.cc b/mindspore/ccsrc/pynative/grad/function/func_grad.cc	(rejected hunks)
@@ -213,15 +213,7 @@ AutoGradMetaData *HasTensorHook(const ValuePtr &value) {
   auto tensor = value->cast<tensor::BaseTensorPtr>();
   if (tensor == nullptr) {
     MS_LOG(DEBUG) << "Hook just work on tensor, not support value " << value->ToString();
-    if (value->isa<ValueSequence>()) {
-      auto seq = value->cast<ValueSequencePtr>();
-      tensor = seq->value()[0]->cast<tensor::BaseTensorPtr>();
-      if (tensor == nullptr) {
-        return nullptr;
-      }
-    } else {
-      return nullptr;
-    }
+    return nullptr;
   }
   auto auto_grad_meta = impl::get_autograd_meta_impl(tensor);
   if (auto_grad_meta == nullptr || auto_grad_meta->backward_hooks().empty()) {
@@ -237,6 +229,9 @@ void RunTensorHook(ValuePtrList *grad_in, AutoGradMetaData *auto_grad_meta) {
                                      kTensorHook, false);
   MS_EXCEPTION_IF_NULL(grad_in);
   MS_EXCEPTION_IF_NULL(auto_grad_meta);
+  if (grad_in->size() != kSizeOne) {
+    MS_LOG(EXCEPTION) << "Tensor hook just work on one tensor value, not support value sequence";
+  }
   runtime::Pipeline::Get().WaitFrontend();
   for (const auto &hook : auto_grad_meta->backward_hooks()) {
     MS_LOG(DEBUG) << "Run hook id T" << hook.first;
@@ -906,7 +901,6 @@ void FuncGrad::CallCustomFunction(const std::shared_ptr<FunctionContext> &contex
   }
   ConstructParameterNodes(context->inputs);
   UpdateNextEdges(custom_fn, context->inputs);
-  custom_fn->set_op_output(PyNativeAlgo::Common::CreateFakeValueWithoutDeviceAddress(context->outputs));
   auto variable = std::make_shared<FuncVariable>(custom_fn, false);
   variable->set_is_custom_op_variable(true);
   (void)variable_set_.insert(variable);
