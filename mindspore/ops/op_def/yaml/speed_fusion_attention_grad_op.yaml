#operator speed_fusion_attention_grad
speed_fusion_attention_grad:
  args:
    query:
      dtype: tensor
    key:
      dtype: tensor
    value:
      dtype: tensor
    dy:
      dtype: tensor
    head_num:
      dtype: int
    input_layout:
      dtype: int
      arg_handler: str_to_enum
    pse:
      dtype: tensor
      default: None
    padding_mask:
      dtype: tensor
      default: None
    atten_mask:
      dtype: tensor
      default: None
    softmax_max:
      dtype: tensor
      default: None
    softmax_sum:
      dtype: tensor
      default: None
    softmax_in:
      dtype: tensor
      default: None
    attention_in:
      dtype: tensor
      default: None
    scale_value:
      dtype: float
      default: 1.0
    keep_prob:
      dtype: float
      default: 1.0
    pre_tokens:
      dtype: int
      default: 2147483647
    next_tokens:
      dtype: int
      default: 2147483647
    inner_precise:
      dtype: int
      default: 0
    seed:
      dtype: tensor
      default: None
    offset:
      dtype: tensor
      default: None
    numels:
      dtype: tensor
      default: None
    prefix:
      dtype: tuple[int]
      type_cast: list[int]
      default: None
    actual_seq_qlen:
      dtype: tuple[int]
      type_cast: list[int]
      default: None
    actual_seq_kvlen:
      dtype: tuple[int]
      type_cast: list[int]
      default: None
    sparse_mode:
      dtype: int
      default: 0
    gen_mask_parallel:
      dtype: bool
      default: True
    sync:
      dtype: bool
      default: False
    pse_type:
      dtype: int
      default: 1
    q_start_idx:
      dtype: tuple[int]
      type_cast: list[int]
      default: None
    kv_start_idx:
      dtype: tuple[int]
      type_cast: list[int]
      default: None
  returns:
    dq:
      dtype: tensor
    dk:
      dtype: tensor
    dv:
      dtype: tensor
    dp:
      dtype: tensor
  dispatch:
    enable: True
    Ascend: SpeedFusionAttentionGradAscend
  function:
    disable: True
