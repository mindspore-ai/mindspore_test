#operator paged_attention
paged_attention:
  args:
    query:
      dtype: tensor
    key_cache:
      dtype: tensor
    value_cache:
      dtype: tensor
      default: None
    block_tables:
      dtype: tensor
      default: None
    context_lens:
      dtype: tensor
      default: None
    antiquant_scale:
      dtype: tensor
      default: None
    antiquant_offset:
      dtype: tensor
      default: None
    attn_mask:
      dtype: tensor
      default: None
    q_seq_lens:
      dtype: tensor
      default: None
    alibi_mask:
      dtype: tensor
      default: None
    head_num:
      dtype: int
      prim_init: True
    scale_value:
      dtype: float
      prim_init: True
    kv_head_num:
      dtype: int
      prim_init: True
    kv_cache_quant_mode:
      dtype: int
      default: "'DEFAULT'"
      prim_init: True
      arg_handler: str_to_enum
    mask_mode:
      dtype: int
      default: "'MASK_DEFAULT'"
      prim_init: True
      arg_handler: str_to_enum
    mla_v_dim:
      dtype: int
      default: 0
      prim_init: True

  returns:
    attention_out:
      dtype: tensor
  function:
    disable: True
