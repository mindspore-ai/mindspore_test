#operator paged_attention_mask
paged_attention_mask:
  args:
    query:
      dtype: tensor
    key_cache:
      dtype: tensor
    value_cache:
      dtype: tensor
    block_tables:
      dtype: tensor
    context_lens:
      dtype: tensor
      type_cast: list[int], tuple[int]
    antiquant_scale:
      dtype: tensor
      default: None
    antiquant_offset:
      dtype: tensor
      default: None
    alibi_mask:
      dtype: tensor
      default: None
    head_num:
      dtype: int
      prim_init: True
    scale_value:
      dtype: float
      prim_init: True
    kv_head_num:
      dtype: int
      prim_init: True
    kv_cache_quant_mode:
      dtype: int
      default: "'DEFAULT'"
      prim_init: True
      arg_handler: str_to_enum

  returns:
    attention_out:
      dtype: tensor
  function:
    disable: True
