#operator flash_attention_score
flash_attention_score:
  args:
    query:
      dtype: tensor
    key:
      dtype: tensor
    value:
      dtype: tensor
    real_shift:
      dtype: tensor
      default: None
    drop_mask:
      dtype: tensor
      default: None
    padding_mask:
      dtype: tensor
      default: None
    attn_mask:
      dtype: tensor
      default: None
    prefix:
      dtype: tuple[int]
      default: None
      type_cast: list[int], tensor
    actual_seq_qlen:
      dtype: tuple[int]
      default: None
      type_cast: list[int], tensor
    actual_seq_kvlen:
      dtype: tuple[int]
      default: None
      type_cast: list[int], tensor
    head_num:
      dtype: int
      prim_init: True
    keep_prob:
      dtype: float
      default: 1.0
      prim_init: True
    scale_value:
      dtype: float
      default: 1.0
      prim_init: True
    pre_tokens:
      dtype: int
      default: 2147483647
      prim_init: True
    next_tokens:
      dtype: int
      default: 2147483647
      prim_init: True
    inner_precise:
      dtype: int
      default: 0
      prim_init: True
    input_layout:
      dtype: int
      default: "'BSH'"
      prim_init: True
      arg_handler: str_to_enum
    sparse_mode:
      dtype: int
      default: 0
      prim_init: True
  returns:
    softmax_max:
      dtype: tensor
    softmax_sum:
      dtype: tensor
    softmax_out:
      dtype: tensor
    attention_out:
      dtype: tensor
  function:
    disable: True
  dispatch:
    enable: True
    Ascend: FlashAttentionScoreAscend
