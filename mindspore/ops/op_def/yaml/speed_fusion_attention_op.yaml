#operator speed_fusion_attention
speed_fusion_attention:
  args:
    query:
      dtype: tensor
    key:
      dtype: tensor
    value:
      dtype: tensor
    head_num:
      dtype: int
    input_layout:
      dtype: int
      arg_handler: str_to_enum
    seed:
      dtype: tensor
    offset:
      dtype: tensor
    pse:
      dtype: tensor
      default: None
    padding_mask:
      dtype: tensor
      default: None
    atten_mask:
      dtype: tensor
      default: None
    scale:
      dtype: float
      default: 1.0
    keep_prob:
      dtype: float
      default: 1.0
    pre_tokens:
      dtype: int
      default: 2147483647
    next_tokens:
      dtype: int
      default: 2147483647
    inner_precise:
      dtype: int
      default: 0
    prefix:
      dtype: tuple[int]
      type_cast: list[int]
      default: None
    actual_seq_qlen:
      dtype: tuple[int]
      type_cast: list[int]
      default: None
    actual_seq_kvlen:
      dtype: tuple[int]
      type_cast: list[int]
      default: None
    sparse_mode:
      dtype: int
      default: 0
    gen_mask_parallel:
      dtype: bool
      default: True
    sync:
      dtype: bool
      default: False
    pse_type:
      dtype: int
      default: 1
    q_start_idx:
      dtype: tuple[int]
      type_cast: list[int]
      default: None
    kv_start_idx:
      dtype: tuple[int]
      type_cast: list[int]
      default: None
  returns:
    attention_out:
      dtype: tensor
    softmax_max:
      dtype: tensor
    softmax_sum:
      dtype: tensor
    softmax_out:
      dtype: tensor
    seed:
      dtype: tensor
    offset:
      dtype: tensor
    numels:
      dtype: tensor
  dispatch:
    enable: True
    Ascend: SpeedFusionAttentionAscend
  function:
    disable: True
