#operator prompt_flash_attention
prompt_flash_attention:
  args:
    query:
      dtype: tensor
    key:
      dtype: tensor
    value:
      dtype: tensor
    attn_mask:
      dtype: tensor
      default: None
    actual_seq_lengths:
      dtype: tuple[int]
      default: None
      type_cast: list[int], tensor
    actual_seq_lengths_kv:
      dtype: tuple[int]
      default: None
      type_cast: list[int], tensor
    pse_shift:
      dtype: tensor
      default: None
    deq_scale1:
      dtype: tensor
      default: None
    quant_scale1:
      dtype: tensor
      default: None
    deq_scale2:
      dtype: tensor
      default: None
    quant_scale2:
      dtype: tensor
      default: None
    quant_offset2:
      dtype: tensor
      default: None
    num_heads:
      dtype: int
      default: 1
      prim_init: True
    scale_value:
      dtype: float
      default: 1.0
      prim_init: True
    pre_tokens:
      dtype: int
      default: 2147483647
      prim_init: True
    next_tokens:
      dtype: int
      default: 0
      prim_init: True
    input_layout:
      dtype: int
      default: "'BSH'"
      prim_init: True
      arg_handler: str_to_enum
    num_key_value_heads:
      dtype: int
      default: 0
      prim_init: True
    sparse_mode:
      dtype: int
      default: 0
      prim_init: True
    inner_precise:
      dtype: int
      default: 1
      prim_init: True
  returns:
    attention_out:
      dtype: tensor
  function:
    disable: True
  dispatch:
    enable: True
    Ascend: PromptFlashAttentionAscend
