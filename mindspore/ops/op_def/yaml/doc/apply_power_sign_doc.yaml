apply_power_sign:
    description: |
        Updates relevant entries according to the AddSign algorithm.

        The AddSign algorithm was proposed in `Neural Optimizer Search with Reinforcement Learning
        <https://arxiv.org/abs/1709.07417>`_.

        .. math::
            \begin{array}{ll} \\
                m_{t+1} = \beta * m_{t} + (1 - \beta) * g \\
                \text{update} = \exp(\text{logbase} * \text{sign_decay} * sign(g) * sign(m)) * g \\
                var = var - lr_{t+1} * \text{update}
            \end{array}

        :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t}`
        is the last moment of :math:`m_{t+1}`, :math:`lr` represents scaling factor `lr`, :math:`g` represents `grad`,
        :math:`\beta` represents `beta`.

        All of inputs comply with the implicit type conversion rules to make the data types consistent.
        If `lr`, `logbase`, `sign_decay` or `beta` is a number, the number is automatically converted to Tensor,
        and the data type is consistent with the Tensor data type involved in the operation.
        If inputs are tensors and have different data types, the lower priority data type will be converted to
        the relatively highest priority data type.

        Note:
            On Ascend, input data type of float64 is currently not supported.

        Inputs:
            - **var** (Union[Parameter, Tensor]) - Variable tensor to be updated. With float64, float32 or float16 data
              type. If data type of `var` is float16, all inputs must have the same data type as `var`.
              The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.
            - **m** (Union[Parameter, Tensor]) - Variable tensor to be updated, has the same shape as `var`.
            - **lr** (Union[Number, Tensor]) - The learning rate value, should be a scalar or Tensor
              with float64, float32 or float16 data type.
            - **logbase** (Union[Number, Tensor]) - Should be a scalar or Tensor with float64, float32 or float16 data type.
            - **sign_decay** (Union[Number, Tensor]) - Should be a scalar or Tensor with float64, float32 or
              float16 data type.
            - **beta** (Union[Number, Tensor]) - The exponential decay rate, should be a scalar or Tensor
              with float64, float32 or float16 data type.
            - **grad** (Tensor) - A tensor of the same shape as `var`, for the gradient.

        Outputs:
            Tuple of 2 Tensors, the updated parameters or tensors.

            - **var** (Tensor) - The same shape and data type as `var`.
            - **m** (Tensor) - The same shape and data type as `m`.

        Raises:
            TypeError: If dtype of `var`, `lr`, `logbase`, `sign_decay`, `beta` or `grad` is not one of float16,
            float32 or float64.
            TypeError: If `lr`, `logbase`, `sign_decay` or `beta` is neither a Number nor a Tensor.
            TypeError: If `grad` is not a Tensor.
            TypeError: If the data type of `lr`, `logbase`, `sign_decay` and `grad` conversion of Parameter
                          is not supported.

        Supported Platforms:
            ``Ascend`` ``GPU`` ``CPU``

        Examples:
            >>> import numpy as np
            >>> from mindspore import Tensor, nn, ops, Parameter
            >>> class Net(nn.Cell):
            ...     def __init__(self):
            ...         super(Net, self).__init__()
            ...         self.apply_power_sign = ops.ApplyPowerSign()
            ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],
            ...                                               [0.1, 0.5]]).astype(np.float32)), name="var")
            ...         self.m = Parameter(Tensor(np.array([[0.6, 0.5],
            ...                                             [0.2, 0.6]]).astype(np.float32)), name="m")
            ...         self.lr = 0.001
            ...         self.logbase = np.e
            ...         self.sign_decay = 0.99
            ...         self.beta = 0.9
            ...     def construct(self, grad):
            ...         out = self.apply_power_sign(self.var, self.m, self.lr, self.logbase,
            ...                                        self.sign_decay, self.beta, grad)
            ...         return out
            ...
            >>> net = Net()
            >>> grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))
            >>> output = net(grad)
            >>> print(output)
            (Tensor(shape=[2, 2], dtype=Float32, value=
            [[ 5.95575690e-01,  3.89676481e-01],
            [ 9.85252112e-02,  4.88201708e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=
            [[ 5.70000052e-01,  5.19999981e-01],
            [ 1.89999998e-01,  6.20000064e-01]]))
