fused_infer_attention_score:
    description: |
        The interface for fully inference.

        .. warning::
            This is an experimental API that is subject to change or deletion.

        Args:
            num_heads (int): The number of heads, equal to `N` when input_layout is `BNSD`.
            scale_value (float): The scale value indicating the scale coefficient, which is used as the scalar of Muls in the calculation.
                Generally, the value is 1.0 / (D ** 0.5). Default: ``1.0``.
            pre_tokens (int): Parameter for sparse computation, represents how many tokens are counted forward.
                Default: ``2147483547``. Invalid when Q_S is 1.
            next_tokens (int): Parameter for sparse computation, represents how many tokens are counted backward.
                Default: ``2147483547``. Invalid when Q_S is 1.
            input_layout (str): Specifies the layout of input `query`, key and value. "BSH", "BNSD" or "BSND" is supported.
                Default "BSH".
            num_key_value_heads (int): head numbers of key/value which are used in GQA algorithm. Default: ``0``.
                The value 0 indicates if the key and value have the same head nums, num_heads must be divisible by num_key_value_heads.
            sparse_mode (int): Indicates sparse mode. Default ``0``.

                - 0: Indicates the defaultMask mode. If attn_mask is not passed, the mask operation is not performed,
                  and preTokens and nextTokens(internally assigned as INT_MAX) are ignored. If passed in, the full attn_mask
                  matrix (S1 * S2) needs to be passed in, indicating that the part between preTokens and nextTokens needs to
                  be calculated.
                - 1: Represents allMask, that is, passing in the complete attn_mask matrix.
                - 2: Representing the leftUpCausal mode corresponds to the lower triangle scenario divided by the left
                  vertex, and the optimized attn_mask matrix (2048*2048) is required.
                - 3: Representing the rightDownCausal model corresponds to the lower triangle scene divided by the lower
                  right vertex, and the optimized attn_mask matrix (2048*2048) is required.
                - 4: Represents the band scenario, that is, the part between counting preTokens and nextTokens, and the
                  optimized attn_mask matrix (2048*2048) is required.
                - 5: Represents the prefix scenario, not implemented yet.
                - 6: Represents the global scenario, not implemented yet.
                - 7: Represents the dilated scenario, not implemented yet.
                - 8: Represents the block_local scenario, not implemented yet.

            inner_precise (int): There are four modes: 0, 1, 2, and 3. Only support 0 and 1 when Q_S is 1. Default: ``1``. 
                - 0: Enable high-precise mode, without row invalid correction.
                - 1: High-performance mode, without row invalid correction.
                - 2: Enable high-precise mode, with row invalid correction.
                - 3: High-performance mode, with row invalid correction.

            block_size (int): Maximum number of tokens per block in the KV cache block for PageAttention. Default: ``0``.
            antiquant_mode (int): Pseudo-quantization mode, 0: per-channel, 1: per-token. This parameter is invalid when Q_S greater than or equal to 2. Default: ``0``.
            softmax_lse_flag (bool): Whether to output softmax_lse. Default: ``False``.

        Inputs:
            - **query** (Tensor) - The query tensor with data type of Int8, float16 or BFloat16.
              Input tensor of shape :math:`(B, S, H)`, :math:`(B, N, S, D)`, or :math:`(B, S, N, D)`.
            - **key** (TensorList) - The key tensor with data type of float16 or BFloat16.
              Input tensor of shape :math:`(B, S, H)`, :math:`(B, N, S, D)`, or :math:`(B, S, N, D)`.
            - **value** (TensorList) - The value tensor with data type of float16 or BFloat16.
              Input tensor of shape :math:`(B, S, H)`, :math:`(B, N, S, D)`, or :math:`(B, S, N, D)`.
            - **pse_shift** (Tensor) - The padding mask tensor with data type of float16 or BFloat16. Default: ``None``.
            - **attn_mask** (Tensor) - The attention mask tensor with data type of int8, uint8 or bool. For each element, 0 indicates retention and 1 indicates discard.
              Default: ``None``.
            - **actual_seq_lengths** (Tensor) - Describe actual sequence length of each input with data type of int64.
              Default: ``None``.
            - **actual_seq_lengths_kv** (Tensor) - Describe actual sequence length of each input with data type of int64.
              Default: ``None``.
            - **dequant_scale1** (Tensor) - Quantization factor for inverse quantization after BMM1 with data type of uint64.
              Default: ``None``.
            - **quant_scale1** (Tensor) - Quantization factors for quantization before BMM2 with data type of float32.
              Default: ``None``.
            - **dequant_scale2** (Tensor) - Quantization factors for quantification after BMM2 with data type of uint64.
              Default: ``None``.
            - **quant_scale2** (Tensor) - Quantization factors for output quantization with data type of float32, BFloat16.
              Default: ``None``.
            - **quant_offset2** (Tensor) - Quantization offset for output quantization with data type of float32, BFloat16.
              Default: ``None``.
            - **antiquant_scale** (Tensor) - Inverse quantization factor with data type of float16, float32, BFloat16. Only support float16 when Q_S greater than or equal to 2.
              Default: ``None``.
            - **antiquant_offset** (Tensor) - Inverse quantization offset with data type of float16, float32, BFloat16. Only support float16 when Q_S greater than or equal to 2.
              Default: ``None``.
            - **block_table** (Tensor) - Block mapping table in KV cache for PageAttention.
              Default: ``None``.
            - **query_padding_size** (Tensor) - Whether each batch of data in the Query is right-aligned. If yes, the number of alignment times is provided. Reserved parameter, not supported yet.
              Default: ``None``.
            - **kv_padding_size** (Tensor) - Whether each batch of data in the Key/Value is right-aligned. If yes, the number of alignment times is provided. Valid only when Q_S is 1.
              Default: ``None``.

        Outputs:
            - **attention_out** (Tensor) - Input tensor, and the shape is :math:`(B, S, H)`, :math:`(B, N, S, D)`, or :math:`(B, S, N, D)`.
            - **softmas_lse** (Tensor[Float32]) - Shape is `(B, N, Q_S, 1)`. The softmas_lse is calculated only if softmax_lse_flag is 1.

        Supported Platforms:
            ``Ascend``

        Examples:
            >>> from mindspore.ops.operations import _infer_ops as infer_ops
            >>> from mindspore import Tensor
            >>> import numpy as np
            >>> B = 1
            >>> N = 16
            >>> S = 256
            >>> D = 16
            >>> query = Tensor(np.ones((B, N, S, D), dtype=np.float16))
            >>> key = [Tensor(np.ones((B, N, S, D), dtype=np.float16))]
            >>> value = [Tensor(np.ones((B, N, S, D), dtype=np.float16))]
            >>> fias = infer_ops.FusedInferAttentionScore(num_heads=N, input_layout='BNSD')
            >>> out = fias(query, key, value, None, None,
            ...            None, None, None, None, None,
            ...            None, None, None, None, None,
            ...            None, None)
            >>> print(out[0].shape)
            (1, 16, 256, 16)
