quant_linear_sparse:
  description: |
    Matmul with a8w8 quant and weight compressed.

    Note:
      - The input `weight` and `compressIdx` should be generated by the compress tool of model_slim.
      - Only support Ascend 310p.

    Inputs:
      x (Tensor): The left matrix with data type of int8. 
      weight (Tensor): The compressed 1-D weight with data type of int8. 
      deqScale (Tensor): The dequant scale with data type of int64. 
      compressIdx (Tensor): The index for decompress weight with data type of int8. 
      bias (Tensor): The bias with data type of int32. 

    Outputs:
        A 2-D Tensor with data type of float16. 

    Supported Platforms:
      ``Ascend``
