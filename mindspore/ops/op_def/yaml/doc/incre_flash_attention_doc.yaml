incre_flash_attention:
  description: |
    The interface for fully inference.

    B -- Batch size

    N -- Num heads

    kvN -- Num key value heads

    S -- Sequence length

    D -- Head dim

    H -- Hidden size

    kvH -- Hidden size of key value

    where :math:`H=N\times D`, :math:`kvH=kvN\times D`

    Self attention constructs an attention model based on the relationship between input samples themselves. The
    principle is to assume that there is a length of the input sample sequence :math:`x` of :math:`n`, and each
    element of :math:`x` is a :math:`d` dimensional vector, which can be viewed as a token embedding. This sequence
    can be transformed through 3 weight matrices to obtain 3 matrices with dimensions of :math:`n\times d`. The self
    attention calculation formula is defined as:

    .. math::
        Attention(Q,K,V)=Softmax(\frac{QK^{T} }{\sqrt{d} } )V

    where the product of :math:`Q` and :math:`K^{T}` represents the attention of input :math:`x`. To avoid the value
    becoming too large, it is usually scaled by dividing it by the square root of :math:`d` and perform softmax
    normalization on each row, yields a matrix of :math:`n\times d` after multiplying :math:`V`.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Note:
      - If there is no input parameter and no default value, None needs to be passed.
      - The shape of the tensor corresponding to the key and value parameters needs to be completely consistent.
      - :math:`N` of parameter query is equal with num_heads. :math:`N` of parameter key and parameter value is equal
        with num_key_value_heads. num_heads is a multiple of num_key_value_heads.
      - Quantization

        - When the data type of query, key, and value is float16 and the data type of output is int8, the input
          parameter quant_scale2 is required and quant_offset2 is optional.
        - When antiquant_scale exists, key and value need to be passed by int8. antiquant_offset is optional.
        - The data type of antiquant_scale and antiquant_offset should be consistency with that of query.
      - pse_shift

        - The pse_shift data type needs to be consistent with the query data type, and only supports D-axis alignment,
          which means that the D-axis can be divided by 16.
      - Page attention:

        - The necessary condition for enabling page attention is that the block_table exists, and the key
          and value are arranged in a contiguous memory according to the index in the block_table. The support for key
          and value dtypes is float16/bfloat16/int8.
        - In the enabling scenario of page attention, 16 alignment is required when input types of key and value are
          float16/bfloat16, and 32 alignment is required when input types of key and value are int8. It is
          recommended to use 128.
        - The maximum max_block_num_per_seq currently supported by block_table is 16k, and exceeding 16k will result
          in interception and error messages; If you encounter :math:`S` being too large and causing
          max_block_num_per_seq to exceed 16k, you can increase the block_size to solve the problem.
        - The multiplication of all dimensions of the shape of the parameters key and value in the page attention
          scenario cannot exceed the representation range of int32.
        - When performing per-channel post quantization, page attention cannot be enabled simultaneously.
      - kv_padding_size:

        - The calculation formula for the starting point of KV cache transfer is
          :math:`S-kv\_padding\_size-actual\_seq\_lengths`. The calculation formula for the transfer endpoint of KV
          cache is :math:`S-kv\_padding\_size`. When the starting or ending point of the KV cache transfer is less
          than 0, the returned data result is all 0.
        - When kv_padding_size is less than 0, it will be set to 0.
        - kv_padding_size needs to be enabled together with the actual_seq_lengths parameter, otherwise it is
          considered as the KV right padding scene.
        - It needs to be enabled together with the atten_mask parameter and ensure that the meaning of atten_mask is
          correct, that is, it can correctly hide invalid data. Otherwise, it will introduce accuracy issues.
        - kv_padding_size does not support page attention scenarios

    Args:
        num_heads (int): The number of heads.
        input_layout (str): the data layout of the input qkv, support 'BSH' and 'BNSD'. Default: ``'BSH'``.
        scale_value (double): The scale value indicating the scale coefficient, which is used as the scalar of
            Muls in the calculation. Default: ``1.0``.
        num_key_value_heads (int): Head numbers of key/value which are used in GQA algorithm.
            The value 0 indicates if the key and value have the same head nums, use num_heads.  Default: ``0``.
        block_size (int): The maximum number of tokens stored in each block of KV in page attention. Default: ``0``.
        inner_precise (int): Default: ``1``.

    Inputs:
        - **query** (Tensor) - The query tensor with data type of float16 or bfloat16.
          The shape is :math:`(B, 1, H)` / :math:`(B, N, 1, D)`.
        - **key** (TensorList) - The key tensor with data type of float16 or bfloat16 or int8.
          The shape is :math:`(B, S, kvH)` / :math:`(B, kvN, S, D)`.
        - **value** (TensorList) - The value tensor with data type of float16 or bfloat16 or int8.
          The shape is :math:`(B, S, kvH)` / :math:`(B, kvN, S, D)`.
        - **attn_mask** (Tensor, optional) - The attention mask tensor with data type of bool or int8 or uint8.
          The shape is :math:`(B, S)` / :math:`(B, 1, S)` / :math:`(B, 1, 1, S)`. Default: ``None``.
        - **actual_seq_lengths** (Union[Tensor, tuple[int], list[int]], optional) - Describe actual sequence length of
          each input with data type of int32 or int64. The shape is :math:`(B, )`. Default: ``None``.
        - **pse_shift** (Tensor, optional) - The position encoding tensor with data type of float16 or bfloat16. Input
          tensor of shape :math:`(1, N, 1, S)` / :math:`(B, N, 1, S)`. Default: ``None``.
        - **dequant_scale1** (Tensor, optional) - Quantitative parametor, the tensor with data type of uint64 or
          float32. It is disable now. Default: ``None``.
        - **quant_scale1** (Tensor, optional) - Quantitative parametor, the tensor with data type of float32. It is
          disable now. Default: ``None``.
        - **dequant_scale2** (Tensor, optional) - Quantitative parametor, the tensor with data type of uint64 or
          float32. It is disable now. Default: ``None``.
        - **quant_scale2** (Tensor, optional) - Post quantitative parametor, the tensor with data type of float32.
          The shape is :math:`(1,)`. Default: ``None``.
        - **quant_offset2** (Tensor, optional) - Post quantitative parametor, the tensor with data type of float32.
          The shape is :math:`(1,)`. Default: ``None``.
        - **antiquant_scale** (Tensor, optional) - Pseudo quantitative parametor, the tensor with data type of float16
          or bfloat16. The shape is :math:`(2, kvN, 1, D)` when input_layout is 'BNSD' or :math:`(2, kvH)` when
          input_layout is 'BSH'. Default: ``None``.
        - **antiquant_offset** (Tensor, optional) - Pseudo quantitative parametor, the tensor with data type of
          float16 or bfloat16. The shape is :math:`(2, kvN, 1, D)` when input_layout is 'BNSD' or :math:`(2, kvH)`
          when input_layout is 'BSH'. Default: ``None``.
        - **block_table** (Tensor, optional) - The tensor with data type of int32. The shape is
          :math:`(B, max\_block\_num\_per\_seq)`, where
          :math:`max\_block\_num\_per\_seq = ceil(\frac{max(actual\_seq\_length)}{block\_size} )`. Default: ``None``.
        - **kv_padding_size** (Tensor, optional) - The tensor with data type of int64. The range of values is
          :math:`0\le kv\_padding\_size \le  S-max(actual\_seq\_length)`. The shape is :math:`()` or :math:`(1,)`.
          Default: ``None``.


    Outputs:
        attention_out (Tensor), the shape is :math:`(B, 1, H)` / :math:`(B, N, 1, D)`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> from mindspore import ops
        >>> from mindspore.common import Tensor
        >>> from mindspore.common import dtype as mstype
        >>> import numpy as np
        >>> from mindspore.ops.auto_generate import IncreFlashAttention
        >>> B, N, S, D, kvN = 1, 4, 10, 128, 1
        >>> query = Tensor(np.random.randn(B, 1, N * D), mstype.float16)
        >>> key = [Tensor(np.random.randn(B, S, kvN * D), mstype.float16)]
        >>> value = [Tensor(np.random.randn(B, S, kvN * D), mstype.float16)]
        >>> ifa_ms = IncreFlashAttention(num_heads=N, num_key_value_heads=kvN)
        >>> attn_out = ifa_ms(query, key, value)
        >>> attn_out
        Tensor(shape=[1, 1, 512], dtype=Float16, value=
        [[[-1.5161e-01, -2.1814e-01, -1.6284e-01 ...  1.0283e+00, -1.1143e+00, -1.7607e+00]]])
