apply_adam_with_amsgrad:
    description: |
        Update var according to the Adam algorithm.

        .. math::
            \begin{array}{l1} \\
                lr_t:=learning\_rate*\sqrt{1-\beta_2^t}/(1-\beta_1^t) \\
                m_t:=\beta_1*m_{t-1}+(1-\beta_1)*g \\
                v_t:=\beta_2*v_{t-1}+(1-\beta_2)*g*g \\
                \hat v_t:=max(\hat v_{t-1}, v_t) \\
                var:=var-lr_t*m_t/(\sqrt{\hat v_t}+\epsilon) \\
            \end{array}

        Inputs of `var`, `m`, `v`, `vhat` and `grad` comply with the implicit type conversion rules
        to make the data types consistent.
        If they have different data types, the lower priority data type will be converted to
        the relatively highest priority data type.

        Inputs of `beta1_power`, `beta1`, `beta2` and `epsilon` comply with the implicit type conversion rules
        to make the data types consistent.
        If they have different data types, the lower priority data type will be converted to
        the relatively highest priority data type.

        However, note that there is no implicit type conversion rule between `var` and `beta1_power`;
        the two sets of rules are independent of each other.

        Args:
            beta1 (float): Must have the same type as beta1_power. Momentum factor. Must be a scalar.
            beta2 (float): Must have the same type as beta1_power. Momentum factor. Must be a scalar.
            epsilon (float): Must have the same type as beta1_power. Ridge term. Must be a scalar.
            use_locking (bool): use_locking: If ``True`` , updating of the `var`, `m`, and `v` tensors will
            be protected by a lock; Otherwise the behavior is undefined, but may exhibit less contention.
            Default: ``False`` .

        Inputs:
            - **var** (Union[Parameter, Tensor]) - Variable to be updated. The data type can be float16 or float32.
            - **m** (Union[Parameter, Tensor]) - The 1st moment vector in the updating formula,
              the shape and data type value should be the same as `var`.
            - **v** (Union[Parameter, Tensor]) - the 2nd moment vector in the updating formula,
              the shape and data type value should be the same as `var`.
            - **vhat** (Union[Parameter, Tensor]) - :math:`\hat v_t` in the updating formula,
              the shape and data type value should be the same as `var`.
            - **beta1_power** (Union[float, Tensor]) - :math:`beta_1^t(\beta_1^{t})` in the updating formula,
              a scalar tensor with float16 or float32 data type.
            - **beta2_power** (Union[float, Tensor]) - :math:`beta_2^t(\beta_2^{t})` in the updating formula,
              a scalar tensor with float16 or float32 data type.
            - **lr** (Union[float, Tensor]) - Scaling factor, a scalar tensor with float16 or float32 data type.
            - **grad** (Tensor) - The gradient, has the same shape and data type as `var`.

        Outputs:
            Tuple of 4 Tensors, the updated parameters or tensors.

            - **var** (Tensor) - The same shape and data type as `var`.
            - **m** (Tensor) - The same shape and data type as `m`.
            - **v** (Tensor) - The same shape and data type as `v`.
            - **vhat** (Tensor) - The same shape and data type as `vhat`.

        Raises:
            TypeError: If `var`, `m`, `v`, `vhat` neither a Parameter nor a Tensor.
            TypeError: If `beta1_power`, `beta2_power`, `lr` is neither a Number nor a Tensor.
            TypeError: If `grad` is not a Tensor.
            TypeError: If dtype of `var`, `m`, `v`, `vhat`, `beta1_power`, `beta2_power`,
                `lr`, `grad`, `momentum` is not float32 or float16.
            ValueError: If `m` or `v` or `vhat` or `grad` doesn't have the same shape of `var`.
            ValueError: If the shape of `beta1_power`, `beta2_power`, `lr` is not 0.

        Supported Platforms:
            ``Ascend`` ``GPU`` ``CPU``

        Examples:
            >>> class ApplyAdamWithAmsgradNet(nn.Cell):
            ...     def __init__(self, beta1=0.9, beta2=0.999, epsilon=1e-8, use_locking=False):
            ...         super(ApplyAdamWithAmsgradNet, self).__init__()
            ...         self.apply_adam_with_amsgrad = P.ApplyAdamWithAmsgrad(beta1, beta2, epsilon, use_locking)
            ...         self.var = Parameter(Tensor(np.array([[0.2, 0.2], [0.2, 0.2]]).astype(np.float32)), name="var")
            ...         self.m = Parameter(Tensor(np.array([[0.1, 0.2], [0.4, 0.3]]).astype(np.float32)), name="m")
            ...         self.v = Parameter(Tensor(np.array([[0.2, 0.1], [0.3, 0.4]]).astype(np.float32)), name="v")
            ...         self.vhat = Parameter(Tensor(np.array([[0.1, 0.2], [0.6, 0.2]]).astype(np.float32)), name="vhat")
            ...     def construct(self, beta1_power, beta2_power, lr, grad):
            ...         out = self.apply_adam_with_amsgrad(self.var, self.m, self.v, self.vhat,
            ...                                            beta1_power, beta2_power, lr, grad)
            ...         return out
            >>> net = ApplyAdamWithAmsgradNet()
            >>> grad = Tensor(np.array([[0.4, 0.2], [0.2, 0.3]]).astype(np.float32))
            >>> output = net(Tensor(0.9, mstype.float32), Tensor(0.999, mstype.float32), Tensor(0.01, mstype.float32), grad)
            >>> print(net.var.asnumpy())
            [[0.19908068 0.1985858 ]
            [0.19844866 0.19849943]]
